<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Information matrices for fitted `lme()` and `gls()` models â€¢ lmeInfo</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><!-- docsearch --><script src="../docsearch.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/docsearch.js/2.6.3/docsearch.min.css" integrity="sha256-QOSRU/ra9ActyXkIBbiIB144aDBdtvXBcNc3OTNuX/Q=" crossorigin="anonymous">
<link href="../docsearch.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script><meta property="og:title" content="Information matrices for fitted `lme()` and `gls()` models">
<meta property="og:description" content="lmeInfo">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">lmeInfo</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.3.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Information-matrices-for-fitted-LME-models.html">Information matrices for fitted `lme()` and `gls()` models</a>
    </li>
    <li>
      <a href="../articles/Standardized-mean-differences-in-multi-level-models.html">Standardized mean differences for fitted `lme` and `gls` models</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/jepusto/lmeInfo" class="external-link">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
<form class="navbar-form navbar-right hidden-xs hidden-sm" role="search">
        <div class="form-group">
          <input type="search" class="form-control" name="search-input" id="search-input" placeholder="Search..." aria-label="Search for..." autocomplete="off">
</div>
      </form>
      
    </div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Information matrices for fitted
<code>lme()</code> and <code>gls()</code> models</h1>
            
            <h4 data-toc-skip class="date">2023-02-15</h4>
      
      
      <div class="hidden name"><code>Information-matrices-for-fitted-LME-models.Rmd</code></div>

    </div>

    
    
<p><span class="math display">\[
\def\bs#1{{\boldsymbol #1}}
\def\bmat#1{{\mathbf #1}}
\def\E{{\text{E}}}
\def\Var{{\text{Var}}}
\def\Cov{{\text{Cov}}}
\def\trace{{\text{tr}}}
\def\Info{{\mathcal{I}}}
\def\Jnfo{{\mathcal{J}}}
\]</span></p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#library(lmeInfo)</span></span></code></pre></div>
<div class="section level2">
<h2 id="notation">Notation<a class="anchor" aria-label="anchor" href="#notation"></a>
</h2>
<p>In what follows, we will use the following symbols for matrix
operations. Let <span class="math inline">\(\bigoplus\)</span> denote
the Kronecker sum, which creates a block-diagonal matrix from a sequence
of sub-matrices. Thus, for matrices <span class="math inline">\(\bmat{A}_1,...,\bmat{A}_m\)</span>, <span class="math display">\[
\bigoplus_{i=1}^m \bmat{A}_i = \left[\begin{array}{cccc}
\bmat{A}_1 &amp; \bmat{0} &amp; \cdots &amp; \bmat{0} \\
\bmat{0} &amp; \bmat{A}_2 &amp;  &amp; \bmat{0} \\
\vdots &amp; &amp; \ddots &amp;  \\
\bmat{0} &amp; \bmat{0} &amp; &amp; \bmat{A}_m \\
\end{array}\right].
\]</span> Let <span class="math inline">\(\bigotimes\)</span> denote the
Kronecker product, such that for <span class="math inline">\(m \times
n\)</span> matrix <span class="math inline">\(\bmat{A}\)</span> and
<span class="math inline">\(f \times g\)</span> matrix <span class="math inline">\(\bmat{B}\)</span>, <span class="math inline">\(\bmat{A} \bigotimes \bmat{B}\)</span> is an <span class="math inline">\((mf) \times (ng)\)</span> matrix: <span class="math display">\[
\bmat{A} \bigotimes \bmat{B} = \left[\begin{array}{cccc}
a_{11} \bmat{B} &amp; a_{12} \bmat{B} &amp; \cdots &amp; a_{1n} \bmat{B}
\\
a_{21} \bmat{B} &amp; a_{22} \bmat{B} &amp; \cdots &amp; a_{2n} \bmat{B}
\\
\vdots &amp; \vdots &amp; \ddots &amp;  \vdots \\
a_{m1} \bmat{B} &amp; a_{m2} \bmat{B} &amp; \cdots &amp; a_{mn} \bmat{B}
\\
\end{array}\right],
\]</span> where <span class="math inline">\(a_{11},...,a_{mn}\)</span>
are the entries of <span class="math inline">\(\bmat{A}\)</span>. In
particular, note that the Kronecker product of an <span class="math inline">\(m \times m\)</span> identity matrix and an
arbitrary matrix <span class="math inline">\(\bmat{B}\)</span> is the
block-diagonal matrix with each of <span class="math inline">\(m\)</span> sub-matrices equal to <span class="math inline">\(\bmat{B}\)</span>: <span class="math display">\[
\bmat{I}_m \bigotimes \bmat{B} = \bigoplus_{i=1}^m \bmat{B}.
\]</span></p>
</div>
<div class="section level2">
<h2 id="hierarchical-linear-models">Hierarchical linear models<a class="anchor" aria-label="anchor" href="#hierarchical-linear-models"></a>
</h2>
<p>We shall be concerned with hierarchical linear models fitted by full
maximum likelihood (FML) or restricted maximum likelihood (RML) using
the <code><a href="https://rdrr.io/pkg/nlme/man/lme.html" class="external-link">lme()</a></code> function from R package <code>nlme</code>.
Consider a set of observations from each of <span class="math inline">\(m\)</span> groups, where group <span class="math inline">\(i\)</span> has <span class="math inline">\(n_i\)</span> observations for <span class="math inline">\(i = 1,...,m\)</span> and <span class="math inline">\(N = \sum_{i=1}^m n_i\)</span>. Let <span class="math inline">\(y_{hi}\)</span> denote the outcome measure and
<span class="math inline">\(\bmat{x}_{hi}\)</span> denote a <span class="math inline">\(p \times 1\)</span> row vector of fixed predictor
variables, both for observation <span class="math inline">\(h\)</span>
from group <span class="math inline">\(i\)</span>. Let <span class="math inline">\(\bmat{y}_i = (y_{1i} \cdots y_{n_i
i})'\)</span> be the <span class="math inline">\(n_i \times
1\)</span> vector of outcomes and <span class="math inline">\(\bmat{X}_i
= \left(\bmat{x}_{1i}' \cdots
\bmat{x}_{n_ii}'\right)'\)</span> be the <span class="math inline">\(n_i \times p\)</span> design matrix of predictors
for group <span class="math inline">\(i\)</span>. Hierarchical
<code>lme</code> models have the form <span class="math display">\[
\bmat{y}_i = \bmat{X}_i \bs\beta + \bmat{Z}_i \bs\eta_i + \bs\epsilon_i
\]</span> where <span class="math inline">\(\bmat{Z}_i\)</span> is a
<span class="math inline">\(n_i \times q_i\)</span> design matrix
describing random effects for group <span class="math inline">\(i\)</span>, <span class="math inline">\(\bs\beta\)</span> is a <span class="math inline">\(p \times 1\)</span> vector of regression
coefficients, <span class="math inline">\(\bs\eta_i\)</span> is a <span class="math inline">\(q_i \times 1\)</span> vector of random effects,
and <span class="math inline">\(\bs\epsilon_i\)</span> is a <span class="math inline">\(n_i \times 1\)</span> vector of
observation-specific errors. We assume that <span class="math display">\[
\bs\eta_i \sim N\left(\bmat{0}, \ \bmat{T}_{i}\right)
\]</span> for <span class="math inline">\(q_i \times q_i\)</span>
covariance matrix <span class="math inline">\(\bmat{T}_i\)</span> and
that <span class="math display">\[
\bs\epsilon_i \sim N\left(\bmat{0}, \ \sigma^2 \bmat{S}_i \bmat{R}_i
\bmat{S}_i \right),
\]</span> where <span class="math inline">\(\sigma^2\)</span> is the
marginal variance of the observation-specific errors, <span class="math inline">\(\bmat{S}_i\)</span> is a diagonal matrix
describing a variance structure, and <span class="math inline">\(\bmat{R}_i\)</span> is a structured correlation
matrix. In <code>lme</code> models, the matrices <span class="math inline">\(\bmat{T}_i\)</span>, <span class="math inline">\(\bmat{S}_i\)</span>, and <span class="math inline">\(\bmat{R}_i\)</span> may be functions of the
unknown parameter vectors <span class="math inline">\(\bs\tau\)</span>
(called the random effects structure parameters), <span class="math inline">\(\bs\psi\)</span> (called the variance structure
parameters), and <span class="math inline">\(\bs\phi\)</span> (called
the correlation structure parameters). For models where the lowest-level
errors are conditionally independent, given the random effects <span class="math inline">\(\bs\eta_i\)</span>, then <span class="math inline">\(\bmat{S}_i = \bmat{R}_i = \bmat{I}_i\)</span>, an
<span class="math inline">\(n_i \times n_i\)</span> identity matrix.</p>
<p>In models with more than one level of random effects (e.g., students
nested in classrooms, nested in schools), the random effects structure
can typically be partitioned into design matrices and random effects
covariance matrices corresponding to each level. For a model with <span class="math inline">\(G\)</span> unique levels of grouping, let <span class="math inline">\(\bmat{Z}_i^{(g)}\)</span> denote the <span class="math inline">\(n_i \times q_{gi}\)</span> design matrix and <span class="math inline">\(\bs\eta_i^{(g)}\)</span> denote the <span class="math inline">\(q_{gi} \times 1\)</span> vector of random effects
corresponding to grouping level <span class="math inline">\(g\)</span>.
Random effects are assumed to be independent across levels, such that
<span class="math display">\[
\bs\eta_i^{(g)} \sim N\left(\bmat{0}, \bmat{T}_i^{(g)}\right),
\]</span> and <span class="math inline">\(\Cov\left(\bs\eta_i^{(g)},
\bs\eta_i^{(h)}\right) = \bmat{0}\)</span> if <span class="math inline">\(g \neq h\)</span>. Let <span class="math inline">\(\bs\tau_g\)</span> be the random effects
parameters corresponding to grouping level <span class="math inline">\(g\)</span>. Thus, the full vector of random
effects is <span class="math inline">\(\bs\eta_i =
\left(\bs\eta_i^{(1)'},...,\bs\eta_i^{(G)'}\right)'\)</span>,
with corresponding design matrix <span class="math inline">\(\bmat{Z}_i
= \left[\bmat{Z}_i^{(1)} \cdots \bmat{Z}_i^{(G)} \right]\)</span>, and
<span class="math display">\[
\bmat{T}_i = \bigoplus_{g=1}^G \bmat{T}_i^{(g)}.
\]</span></p>
<p>Under this model, the marginal distribution of <span class="math inline">\(\bmat{y}_i\)</span> is <span class="math display">\[
\left(\bmat{y}_i | \bmat{X}_i\right) \sim N\left(\bmat{X}_i \bs\beta, \
\bmat{V}_i \right),
\]</span> where the marginal variance-covariance matrix for group <span class="math inline">\(i\)</span> is <span class="math display">\[
\bmat{V}_i = \bmat{Z}_i \bmat{T}_i \bmat{Z}_i' + \sigma^2 \bmat{S}_i
\bmat{R}_i\bmat{S}_i = \sum_{g=1}^G \bmat{Z}_i^{(g)} \bmat{T}_i^{(g)}
\bmat{Z}_i^{(g)'} + \sigma^2 \bmat{S}_i \bmat{R}_i\bmat{S}_i,
\]</span> for <span class="math inline">\(i = 1,...,m\)</span>.</p>
<p>For estimation purposes, it will be convenient to use notation for
the full data vectors. Let <span class="math inline">\(\bmat{y} =
\left(\bmat{y}_1' \cdots \bmat{y}_m'\right)'\)</span>, <span class="math inline">\(\bmat{X} = \left(\bmat{X}_1' \cdots
\bmat{X}_m'\right)'\)</span>, and <span class="math inline">\(\bmat{Z} = \bigoplus_{i=1}^m \bmat{Z}_i\)</span>.
Let <span class="math inline">\(\bmat{V} = \bigoplus_{i=1}^m
\bmat{V}_i\)</span>, with <span class="math inline">\(\bmat{T}\)</span>,
<span class="math inline">\(\bmat{S}\)</span>, and <span class="math inline">\(\bmat{R}\)</span> similarly defined, so that <span class="math display">\[
\bmat{V} = \bmat{Z} \bmat{T} \bmat{Z}' + \sigma^2 \bmat{S} \bmat{R}
\bmat{S}
\]</span></p>
</div>
<div class="section level2">
<h2 id="estimation">Estimation<a class="anchor" aria-label="anchor" href="#estimation"></a>
</h2>
<p>Fitting hierarchical models by FML or RML entails estimating both the
fixed effect coefficients <span class="math inline">\(\bs\beta\)</span>
and the parameters of the random effects structure, variance structure,
and correlation structure. Let <span class="math inline">\(\bs\theta =
\left(\bs\tau', \bs\psi', \bs\phi',\sigma^2
\right)'\)</span> denote the vector collecting of all of the latter
parameters, with a total of <span class="math inline">\(r\)</span>
unique entries.</p>
<div class="section level3">
<h3 id="fixed-effect-estimation">Fixed effect estimation<a class="anchor" aria-label="anchor" href="#fixed-effect-estimation"></a>
</h3>
<p>For explanatory purposes, it is helpful to begin by considering
estimation of the fixed effects, supposing that <span class="math inline">\(\bs\theta\)</span> is known (and thus, that that
the marginal variance-covariances <span class="math inline">\(\bmat{V}_i\)</span> are known). In this case, the
only unknowns are the fixed effects <span class="math inline">\(\bs\beta\)</span>, which can be estimated
efficiently using weighted least squares (WLS). The WLS estimator of
<span class="math inline">\(\bs\beta\)</span> is given by <span class="math display">\[
\hat{\bs\beta} = \bmat{M} \bmat{X}' \bmat{V}^{-1} \bmat{y}, \qquad
\text{where} \qquad \bmat{M} = \left(\bmat{X}' \bmat{V}^{-1}
\bmat{X}\right)^{-1}.
\]</span> Assuming that the variance parameters are known, the sampling
distribution of <span class="math inline">\(\hat{\bs\beta}\)</span> is
multivariate normal with mean <span class="math inline">\(\bs\beta\)</span> and covariance matrix <span class="math display">\[
\Var\left(\hat{\bs\beta}\right) = \bmat{M}.
\]</span> Of course, in practice, the variance parameters must be
estimated. Feasible WLS thus uses estimates of the variance parameters,
<span class="math inline">\(\hat{\bs\theta}\)</span>, to calculate an
estimate <span class="math inline">\(\hat{\bmat{V}} =
\bmat{V}(\hat{\bs\theta})\)</span> that is used in place of <span class="math inline">\(\bmat{V}\)</span> above. Thus, the estimated
sampling covariance matrix of <span class="math inline">\(\hat{\bs\beta}\)</span> is <span class="math display">\[
\hat{\bmat{M}} = \bmat{M}(\hat{\bs\theta}) = \left(\bmat{X}'
\hat{\bmat{V}}^{-1} \bmat{X}\right)^{-1}.
\]</span> It is known that <span class="math inline">\(\hat{\bmat{M}}\)</span> tends to underestimate the
true covariance of <span class="math inline">\(\hat{\bs\beta}\)</span>
when <span class="math inline">\(m\)</span> is small. Kenward and Roger
<span class="citation">(1997, 2009)</span> proposed more elaborate
covariance estimators and hypothesis testing procedures for use in small
samples.</p>
</div>
<div class="section level3">
<h3 id="variance-parameter-estimation">Variance parameter estimation<a class="anchor" aria-label="anchor" href="#variance-parameter-estimation"></a>
</h3>
<p>The feasible WLS estimator is based on estimates of the variance
parameters. In <code>lme</code>, FML and RML estimators for these
parameters are obtained by maximizing the log likelihood or restricted
log likelihood of the model via iterative numerical methods. Following
<span class="citation">Lindstrom &amp; Bates (1988)</span>, -2 times the
full log likelihood is given by <span class="math display">\[
-2 l_F\left(\bs\beta, \bs\theta\right) = \log
\left|\bmat{V}(\bs\theta)\right| + \bmat{r}'
\bmat{V}^{-1}(\bs\theta)\bmat{r},
\]</span> where <span class="math inline">\(\bmat{r} = \bmat{y} -
\bmat{X} \bs\beta\)</span> and <span class="math inline">\(\left| \cdot
\right|\)</span> denotes the norm of a matrix. Similarly, -2 times the
restricted log likelihood (which is a function of <span class="math inline">\(\bs\theta\)</span> alone) is given by <span class="math display">\[
-2 l_R\left(\bs\theta\right) = \log \left|\bmat{X}'\bmat{V}^{-1}
\bmat{X} \right| +  \log \left|\bmat{V}(\bs\theta)\right| +
\bmat{y}' \bmat{Q}(\bs\theta)\bmat{y},
\]</span> where <span class="math display">\[
\bmat{Q}(\bs\theta) = \bmat{V}^{-1}(\bs\theta) -
\bmat{V}^{-1}(\bs\theta) \bmat{X} \left(\bmat{X}'
\bmat{V}^{-1}(\bs\theta) \bmat{X}\right)^{-1} \bmat{X}'
\bmat{V}^{-1}(\bs\theta).
\]</span> Let <span class="math inline">\(\hat{\bs\theta}_F\)</span> and
<span class="math inline">\(\hat{\bs\theta}_R\)</span> denote the FML
and RML estimators of the variance parameters, respectively. Let <span class="math inline">\(\hat{\bs\theta}\)</span> be a generic estimator of
the variance parameters (i.e., either the FML or RML estimator).</p>
</div>
<div class="section level3">
<h3 id="sampling-variance-of-variance-parameters">Sampling variance of variance parameters<a class="anchor" aria-label="anchor" href="#sampling-variance-of-variance-parameters"></a>
</h3>
<p>The analyst might need to obtain estimates of the uncertainty in
<span class="math inline">\(\hat{\bs\theta}\)</span>, either for
purposes of inference or as a component of small-sample approximations
for other statistics. For purposes of inference, a recommended approach
to obtain a confidence interval for a single component of <span class="math inline">\(\bs\theta\)</span> is to use profile likelihood
methods. Another approach is to use approximations based on the
information matrix of the full or restricted likelihood. The inverse of
the observed, expected, or average Fisher information provides an
approximate estimate of <span class="math inline">\(\Var(\hat{\bs\theta})\)</span>, valid as the
number of groups grows large. Thus, define <span class="math display">\[
\bmat{C}(\hat{\bs\theta}) = \Info^{-1},
\]</span> where <span class="math inline">\(\Info\)</span> is the
observed, expected, or average information matrix. We now define these
matrices in detail.</p>
<div class="section level4">
<h4 id="restricted-maximum-likelihood">Restricted maximum likelihood<a class="anchor" aria-label="anchor" href="#restricted-maximum-likelihood"></a>
</h4>
<p>The observed information is the negative Hessian matrix (-1 times the
matrix of second derivatives) of the log likelihood, evaluated using the
full or restricted maximum likelihood estimator of <span class="math inline">\(\theta\)</span>. For RML estimators, the observed
information matrix has entries <span class="math display">\[
\begin{aligned}
\Info^{RO}_{st} &amp;= \left. -\frac{\partial^2 l_R(\bs\theta)}{\partial
\theta_s \partial \theta_t} \right|_{\bs\theta = \hat{\bs\theta}} \\
&amp;= \frac{1}{2}
\bmat{y}'\bmat{Q}\left(\dot{\bmat{V}}_s\bmat{Q}\dot{\bmat{V}}_t +
\dot{\bmat{V}}_t\bmat{Q}\dot{\bmat{V}}_s - \ddot{\bmat{V}}_{st}\right)
\bmat{Q}\bmat{y} -
\frac{1}{2}\trace\left(\bmat{Q}\dot{\bmat{V}}_s\bmat{Q}\dot{\bmat{V}}_t
- \bmat{Q}\ddot{\bmat{V}}_{st}\right) \\
&amp;= \frac{1}{2}
\hat{\bmat{r}}'\hat{\bmat{V}}^{-1}\left(\dot{\bmat{V}}_s\bmat{Q}\dot{\bmat{V}}_t
+ \dot{\bmat{V}}_t\bmat{Q}\dot{\bmat{V}}_s - \ddot{\bmat{V}}_{st}\right)
\hat{\bmat{V}}^{-1}\hat{\bmat{r}} -
\frac{1}{2}\trace\left(\bmat{Q}\dot{\bmat{V}}_s\bmat{Q}\dot{\bmat{V}}_t
- \bmat{Q}\ddot{\bmat{V}}_{st}\right)
\end{aligned}
\]</span> for <span class="math inline">\(s,t = 1,...,r\)</span>, where
<span class="math inline">\(\dot{\bmat{V}}_s = \left. \partial \bmat{V}
/ \partial \theta_s \right|_{\bs\theta = \hat{\bs\theta}}\)</span>,
<span class="math inline">\(\ddot{\bmat{V}}_{st} = \left. \partial^2
\bmat{V} / \partial \theta_s \partial \theta_t \right|_{\bs\theta =
\hat{\bs\theta}}\)</span>, and <span class="math inline">\(\bmat{Q} =
\bmat{Q}(\hat{\bs\theta})\)</span>.</p>
<p>The expected information matrix is the expected value of <span class="math inline">\(\Info^{RO}\)</span> over the distribution of <span class="math inline">\(\bmat{y}\)</span>, evaluated at the maximum
likelihood estimator of <span class="math inline">\(\theta\)</span>. For
RML estimators, the expected information has entries <span class="math display">\[
\Info^{RE}_{st} =
\frac{1}{2}\trace\left(\bmat{Q}\dot{\bmat{V}}_s\bmat{Q}\dot{\bmat{V}}_t\right)
\]</span> for <span class="math inline">\(s,t = 1,...,r\)</span>.</p>
<p>The average information matrix is the average of <span class="math inline">\(\Info^{RO}\)</span> and <span class="math inline">\(\Info^{RE}\)</span>, with the terms involving
second derivatives of <span class="math inline">\(\bmat{V}\)</span>
approximated by their expectations <span class="citation">(Gilmour,
Thompson, &amp; Cullis, 1995)</span>. For RML estimators, the entries
are given by <span class="math display">\[
\Info^{RA}_{st} = \frac{1}{2}
\bmat{y}'\bmat{Q}\dot{\bmat{V}}_s\bmat{Q}\dot{\bmat{V}}_t\bmat{Q}\bmat{y}
= \frac{1}{2}
\hat{\bmat{r}}'\hat{\bmat{V}}^{-1}\dot{\bmat{V}}_s\bmat{Q}\dot{\bmat{V}}_t
\hat{\bmat{V}}^{-1}\hat{\bmat{r}},
\]</span> for <span class="math inline">\(s,t = 1,...,r\)</span>, where
<span class="math inline">\(\hat{\bmat{r}} = \bmat{y} - \bmat{X}
\hat{\bs\beta}\)</span>. The average information matrix is used by the
program <code>ASReml</code> <span class="citation">(Gilmour, Gogel,
Cullis, &amp; Thompson, 2009)</span> due to its computational efficiency
for models with large sample sizes.</p>
<p>Numerical optimization algorithms will often use a parameterization
of the model that differs from the parameter definitions of interest.
For example, we might want to approximate the variance of a sum of
several variance components in their natural parameterization, but the
optimization algorithm may use a log likelihood defined in terms of the
natural logs of standard deviations. Thus, we will sometimes need to
convert information matrices from one parameterization to another.
Suppose that we have the information matrix calculated in terms of a
parameter <span class="math inline">\(\bs\xi = g(\bs\theta)\)</span>,
where <span class="math inline">\(g()\)</span> is a one-to-one function
with inverse <span class="math inline">\(h()\)</span>. Define the
Jacobian matrix of the inverse as <span class="math display">\[
\nabla_\xi h = \left[\frac{\partial h_s}{\partial \xi_t} \right]_{s,t =
1,...,r}.
\]</span> Let <span class="math inline">\(\Jnfo\)</span> denote the
information matrix (observed, expected, or average) in the <span class="math inline">\(\bs\xi\)</span> parameterization. An approximate
covariance matrix for the maximum likelihood estimator <span class="math inline">\(\hat{\bs\theta}\)</span> is <span class="math display">\[
\bmat{C}(\hat{\bs\theta}) = \left(\nabla_\xi h \right)
\left[\Jnfo^{-1}\right] \left(\nabla_\xi h \right)',
\]</span> where <span class="math inline">\(\nabla_\xi h\)</span> is
evaluated at <span class="math inline">\(\hat{\bs\xi}\)</span>.</p>
</div>
<div class="section level4">
<h4 id="full-maximum-likelihood">Full maximum likelihood<a class="anchor" aria-label="anchor" href="#full-maximum-likelihood"></a>
</h4>
<p>For FML estimators, the entries of the observed information matrix
involve <span class="math inline">\(\bs\beta\)</span> in addition to
<span class="math inline">\(\bs\theta\)</span>. The entries for <span class="math inline">\(\bs\theta\)</span> are <span class="math display">\[
\begin{aligned}
\Info^{FO}_{st} &amp;= \left. -\frac{\partial^2 l_F(\bs\beta,
\bs\theta)}{\partial \theta_s \partial \theta_t} \right|_{\bs\theta =
\hat{\bs\theta}} \\
&amp;= \frac{1}{2}
\hat{\bmat{r}}'\hat{\bmat{V}}^{-1}\left(\dot{\bmat{V}}_s\hat{\bmat{V}}^{-1}\dot{\bmat{V}}_t
+ \dot{\bmat{V}}_t\hat{\bmat{V}}^{-1}\dot{\bmat{V}}_s -
\ddot{\bmat{V}}_{st}\right) \hat{\bmat{V}}^{-1}\hat{\bmat{r}} -
\frac{1}{2}\trace\left(\hat{\bmat{V}}^{-1}\dot{\bmat{V}}_s\hat{\bmat{V}}^{-1}\dot{\bmat{V}}_t
- \hat{\bmat{V}}^{-1}\ddot{\bmat{V}}_{st}\right).
\end{aligned}
\]</span> The cross-derivatives involving <span class="math inline">\(\bs\beta\)</span> are <span class="math display">\[
\Info^{FO}_{s \bs\beta'} = \left. -\frac{\partial^2 l_F(\bs\beta,
\bs\theta)}{\partial \theta_s \partial\bs\beta'} \right|_{\bs\theta
= \hat{\bs\theta}} = \hat{\bmat{r}}' \hat{\bmat{V}}^{-1}
\dot{\bmat{V}}_s \hat{\bmat{V}}^{-1} \bmat{X},
\]</span> which have expectation <span class="math inline">\(\bmat{0}'\)</span> and thus might be ignored,
so that the sampling variance of <span class="math inline">\(\hat{\bs\theta}\)</span> would be approximated by
<span class="math inline">\(\bmat{C}(\hat{\bs\theta}) =
\left(\bmat{I}^{FO}_{\bs\theta\bs\theta'}\right)^{-1}\)</span>.</p>
<p>The expected information matrix for the FML estimator is simply <span class="math display">\[
\Info^{FE}_{st} =
\frac{1}{2}\trace\left(\hat{\bmat{V}}^{-1}\dot{\bmat{V}}_s
\hat{\bmat{V}}^{-1} \dot{\bmat{V}}_t\right)
\]</span> and the average information matrix is <span class="math display">\[
\Info^{FA}_{st} =
\frac{1}{2}\hat{\bmat{r}}'\hat{\bmat{V}}^{-1}\dot{\bmat{V}}_s
\hat{\bmat{V}}^{-1} \dot{\bmat{V}}_t \hat{\bmat{V}}^{-1} \hat{\bmat{r}}.
\]</span></p>
</div>
</div>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-Gilmour2009ASReml" class="csl-entry">
Gilmour, A. R., Gogel, B. J., Cullis, B. R., &amp; Thompson, R. (2009).
<em><span>ASReml User Guide</span></em>.
</div>
<div id="ref-Gilmour1995average" class="csl-entry">
Gilmour, A. R., Thompson, R., &amp; Cullis, B. R. (1995). Average
information <span>REML</span>: <span>An</span> efficient algorithm for
variance parameter estimation in linear mixed models.
<em>Biometrics</em>, <em>51</em>(4), 1440â€“1450. <a href="https://doi.org/10.2307/2533274" class="external-link">https://doi.org/10.2307/2533274</a>
</div>
<div id="ref-Kenward1997small" class="csl-entry">
Kenward, M. G., &amp; Roger, J. H. (1997). Small sample inference for
fixed effects from restricted maximum likelihood. <em>Biometrics</em>,
<em>53</em>(3), 983â€“997. <a href="https://doi.org/10.2307/2533558" class="external-link">https://doi.org/10.2307/2533558</a>
</div>
<div id="ref-Kenward2009improved" class="csl-entry">
Kenward, M. G., &amp; Roger, J. H. (2009). An improved approximation to
the precision of fixed effects from restricted maximum likelihood.
<em>Computational Statistics &amp; Data Analysis</em>, <em>53</em>(7),
2583â€“2595. <a href="https://doi.org/10.1016/j.csda.2008.12.013" class="external-link">https://doi.org/10.1016/j.csda.2008.12.013</a>
</div>
<div id="ref-Lindstrom1988" class="csl-entry">
Lindstrom, M. J., &amp; Bates, D. M. (1988). Newton-<span>Raphson</span>
and <span>EM</span> algorithms for linear mixed-effects models for
repeated-measures data. <em>Journal of the American Statistical
Association</em>, <em>83</em>(404), 1014â€“1022. <a href="https://doi.org/10.1080/01621459.1988.10478693" class="external-link">https://doi.org/10.1080/01621459.1988.10478693</a>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by <a href="https://www.jepusto.com/" class="external-link">James Pustejovsky</a>, Man Chen.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/docsearch.js/2.6.1/docsearch.min.js" integrity="sha256-GKvGqXDznoRYHCwKXGnuchvKSwmx9SRMrZOTh2g4Sb0=" crossorigin="anonymous"></script><script>
  docsearch({
    
    
    apiKey: 'ab49bdeb500af0e2e09f15c826d29f1f',
    indexName: 'lmeInfo',
    inputSelector: 'input#search-input.form-control',
    transformData: function(hits) {
      return hits.map(function (hit) {
        hit.url = updateHitURL(hit);
        return hit;
      });
    }
  });
</script>
</body>
</html>
